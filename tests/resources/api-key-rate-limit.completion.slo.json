{
    "api_base": "",
    "cache_hit": false,
    "cache_key": null,
    "call_type": "acompletion",
    "completionStartTime": 1763037988.533372,
    "completion_tokens": 0,
    "cost_breakdown": null,
    "custom_llm_provider": null,
    "endTime": 1763037988.533372,
    "end_user": "",
    "error_information": {
        "error_class": "HTTPException",
        "error_code": "429",
        "error_message": "429: Rate limit exceeded for api_key: 5c4e0b24628de73df9e811623c4239f1e4f5157dd15faa1d0e20f31271071c1d. Limit type: requests. Current limit: 2, Remaining: 0. Limit resets at: 2025-11-13 12:47:28 UTC",
        "llm_provider": "",
        "traceback": "  File \"/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py\", line 4686, in chat_completion\n    result = await base_llm_response_processor.base_process_llm_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<16 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/common_request_processing.py\", line 448, in base_process_llm_request\n    self.data, logging_obj = await self.common_processing_pre_call_logic(\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<13 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/common_request_processing.py\", line 386, in common_processing_pre_call_logic\n    self.data = await proxy_logging_obj.pre_call_hook(  # type: ignore\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        user_api_key_dict=user_api_key_dict, data=self.data, call_type=route_type  # type: ignore\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 1035, in pre_call_hook\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/utils.py\", line 1009, in pre_call_hook\n    response = await _callback.async_pre_call_hook(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/hooks/parallel_request_limiter_v3.py\", line 1193, in async_pre_call_hook\n    self._handle_rate_limit_error(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        response=response,\n        ^^^^^^^^^^^^^^^^^^\n        descriptors=descriptors,\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/proxy/hooks/parallel_request_limiter_v3.py\", line 1106, in _handle_rate_limit_error\n    raise HTTPException(\n    ...<7 lines>...\n    )\n"
    },
    "error_str": "429: Rate limit exceeded for api_key: 5c4e0b24628de73df9e811623c4239f1e4f5157dd15faa1d0e20f31271071c1d. Limit type: requests. Current limit: 2, Remaining: 0. Limit resets at: 2025-11-13 12:47:28 UTC",
    "guardrail_information": null,
    "hidden_params": {
        "additional_headers": {},
        "api_base": null,
        "batch_models": null,
        "cache_key": null,
        "litellm_model_name": null,
        "litellm_overhead_time_ms": null,
        "model_id": null,
        "response_cost": null,
        "usage_object": null
    },
    "id": "e5edf587-849e-4a52-b66e-8a8945683cde",
    "messages": [
        {
            "content": "redacted-by-litellm",
            "role": "user"
        }
    ],
    "metadata": {
        "applied_guardrails": [],
        "cold_storage_object_key": null,
        "mcp_tool_call_metadata": null,
        "prompt_management_metadata": null,
        "requester_custom_headers": {},
        "requester_ip_address": "",
        "requester_metadata": {},
        "spend_logs_metadata": null,
        "usage_object": {
            "completion_tokens": 0,
            "completion_tokens_details": null,
            "prompt_tokens": 0,
            "prompt_tokens_details": null,
            "total_tokens": 0
        },
        "user_api_key_alias": "filipe-test-6",
        "user_api_key_auth_metadata": {
            "key1": "val1",
            "logging": [],
            "rpm_limit_type": "best_effort_throughput",
            "tpm_limit_type": "best_effort_throughput"
        },
        "user_api_key_budget_reset_at": null,
        "user_api_key_end_user_id": null,
        "user_api_key_hash": "5c4e0b24628de73df9e811623c4239f1e4f5157dd15faa1d0e20f31271071c1d",
        "user_api_key_max_budget": null,
        "user_api_key_org_id": null,
        "user_api_key_request_route": "/v1/chat/completions",
        "user_api_key_spend": 0.0223069,
        "user_api_key_team_alias": null,
        "user_api_key_team_id": null,
        "user_api_key_user_email": "filipe+litellm5@amberflo.io",
        "user_api_key_user_id": "dc996d2a-b1e0-4e6b-9268-07e0a2adb585",
        "vector_store_request_metadata": null
    },
    "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
    "model_group": "",
    "model_id": "",
    "model_map_information": {
        "model_map_key": "",
        "model_map_value": null
    },
    "model_parameters": {},
    "prompt_tokens": 0,
    "request_tags": [
        "User-Agent: curl",
        "User-Agent: curl/7.88.1"
    ],
    "requester_ip_address": "",
    "response": {
        "text": "redacted-by-litellm"
    },
    "response_cost": 0.0,
    "response_cost_failure_debug_info": null,
    "response_time": 0.001689910888671875,
    "saved_cache_cost": 0.0,
    "standard_built_in_tools_params": {
        "file_search": null,
        "web_search_options": null
    },
    "startTime": 1763037988.531682,
    "status": "failure",
    "status_fields": {
        "guardrail_status": "not_run",
        "llm_api_status": "failure"
    },
    "stream": null,
    "total_tokens": 0,
    "trace_id": "ee35f459-503f-4a27-b61d-12e4433aada7"
}
